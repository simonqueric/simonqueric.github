---
title: 'Information theory'
date: 2024-01-27
permalink: /posts/2024/01/27/blog-post-1/
tags:
  - test
---

The building block of Information Theory of Claude Shannon is the entropy of a random variable $X$. Intuitively, it's a measure of the information carried by $X$. It's defined as : 

$$ H(X) = -\sum_{x\in\mathcal{X}}{p(x)\log{p(x)}}$$

Let's break down this expression a little bit. For a given outcome $x$, if its probability is low, then it's unexpected to see it so that it carries a lot of information. At the other end of the spectrum, an outcome with a high probability doesn't carry any information/ A good quantity for this information would be $-\log{p(x)}$. 

In the expression above, we can see that the entropy of $X$ is the average of the individual information measures of every outcome so that we can rewrite the expression to get :

$$ H(X) = -\mathbb{E}[\log{p(X)}] $$
